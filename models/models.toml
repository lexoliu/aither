# Model Registry Database
# This file contains metadata for popular LLM models.
# Edit this file to add or update models.

# =============================================================================
# OpenAI Models
# =============================================================================

[[models]]
id = "gpt-4o"
aliases = ["gpt-4o-2024-05-13", "gpt-4o-2024-08-06", "gpt-4o-2024-11-20"]
name = "GPT-4o"
provider = "openai"
context_window = 128_000
max_output_tokens = 16_384
tier = "balanced"
capabilities = ["vision", "tool_use", "audio"]

[[models]]
id = "gpt-4o-mini"
aliases = ["gpt-4o-mini-2024-07-18"]
name = "GPT-4o Mini"
provider = "openai"
context_window = 128_000
max_output_tokens = 16_384
tier = "fast"
capabilities = ["vision", "tool_use"]

[[models]]
id = "gpt-4-turbo"
aliases = ["gpt-4-turbo-2024-04-09", "gpt-4-turbo-preview"]
name = "GPT-4 Turbo"
provider = "openai"
context_window = 128_000
max_output_tokens = 4_096
tier = "flagship"
capabilities = ["vision", "tool_use"]

[[models]]
id = "gpt-4"
aliases = ["gpt-4-0613", "gpt-4-0314"]
name = "GPT-4"
provider = "openai"
context_window = 8_192
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["tool_use"]

[[models]]
id = "gpt-4-32k"
aliases = ["gpt-4-32k-0613"]
name = "GPT-4 32K"
provider = "openai"
context_window = 32_768
max_output_tokens = 32_768
tier = "flagship"
capabilities = ["tool_use"]

[[models]]
id = "gpt-3.5-turbo"
aliases = ["gpt-3.5-turbo-0125", "gpt-3.5-turbo-1106"]
name = "GPT-3.5 Turbo"
provider = "openai"
context_window = 16_385
max_output_tokens = 4_096
tier = "fast"
capabilities = ["tool_use"]

[[models]]
id = "o1"
aliases = ["o1-2024-12-17"]
name = "O1"
provider = "openai"
context_window = 200_000
max_output_tokens = 100_000
tier = "flagship"
capabilities = ["vision", "tool_use", "reasoning"]

[[models]]
id = "o1-mini"
aliases = ["o1-mini-2024-09-12"]
name = "O1 Mini"
provider = "openai"
context_window = 128_000
max_output_tokens = 65_536
tier = "balanced"
capabilities = ["reasoning"]

[[models]]
id = "o1-pro"
aliases = []
name = "O1 Pro"
provider = "openai"
context_window = 200_000
max_output_tokens = 100_000
tier = "flagship"
capabilities = ["vision", "tool_use", "reasoning"]

[[models]]
id = "o3"
aliases = []
name = "O3"
provider = "openai"
context_window = 200_000
max_output_tokens = 100_000
tier = "flagship"
capabilities = ["vision", "tool_use", "reasoning"]

[[models]]
id = "o3-mini"
aliases = ["o3-mini-2025-01-31"]
name = "O3 Mini"
provider = "openai"
context_window = 200_000
max_output_tokens = 100_000
tier = "balanced"
capabilities = ["tool_use", "reasoning"]

[[models]]
id = "o4-mini"
aliases = []
name = "O4 Mini"
provider = "openai"
context_window = 200_000
max_output_tokens = 100_000
tier = "balanced"
capabilities = ["vision", "tool_use", "reasoning"]

# =============================================================================
# Anthropic Claude Models
# =============================================================================

[[models]]
id = "claude-opus-4"
aliases = ["claude-opus-4-20250514"]
name = "Claude Opus 4"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 32_000
tier = "flagship"
capabilities = ["vision", "tool_use", "pdf"]

[[models]]
id = "claude-sonnet-4"
aliases = ["claude-sonnet-4-20250514"]
name = "Claude Sonnet 4"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 64_000
tier = "balanced"
capabilities = ["vision", "tool_use", "pdf"]

[[models]]
id = "claude-3-7-sonnet"
aliases = ["claude-3-7-sonnet-20250219"]
name = "Claude 3.7 Sonnet"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 64_000
tier = "balanced"
capabilities = ["vision", "tool_use", "pdf", "reasoning"]

[[models]]
id = "claude-3-5-sonnet"
aliases = ["claude-3-5-sonnet-20241022", "claude-3-5-sonnet-20240620", "claude-3-5-sonnet-v2"]
name = "Claude 3.5 Sonnet"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["vision", "tool_use", "pdf"]

[[models]]
id = "claude-3-5-haiku"
aliases = ["claude-3-5-haiku-20241022"]
name = "Claude 3.5 Haiku"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 8_192
tier = "fast"
capabilities = ["vision", "tool_use"]

[[models]]
id = "claude-3-opus"
aliases = ["claude-3-opus-20240229"]
name = "Claude 3 Opus"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 4_096
tier = "flagship"
capabilities = ["vision", "tool_use", "pdf"]

[[models]]
id = "claude-3-sonnet"
aliases = ["claude-3-sonnet-20240229"]
name = "Claude 3 Sonnet"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 4_096
tier = "balanced"
capabilities = ["vision", "tool_use"]

[[models]]
id = "claude-3-haiku"
aliases = ["claude-3-haiku-20240307"]
name = "Claude 3 Haiku"
provider = "anthropic"
context_window = 200_000
max_output_tokens = 4_096
tier = "fast"
capabilities = ["vision", "tool_use"]

[[models]]
id = "claude-2"
aliases = ["claude-2.1", "claude-2.0"]
name = "Claude 2"
provider = "anthropic"
context_window = 100_000
max_output_tokens = 4_096
tier = "balanced"
capabilities = []

# =============================================================================
# Google Gemini Models
# =============================================================================

[[models]]
id = "gemini-2.5-pro"
aliases = ["gemini-2.5-pro-preview-05-06"]
name = "Gemini 2.5 Pro"
provider = "google"
context_window = 1_048_576
max_output_tokens = 65_536
tier = "flagship"
capabilities = ["vision", "tool_use", "audio", "video", "pdf", "reasoning"]

[[models]]
id = "gemini-2.5-flash"
aliases = ["gemini-2.5-flash-preview-04-17"]
name = "Gemini 2.5 Flash"
provider = "google"
context_window = 1_048_576
max_output_tokens = 65_536
tier = "balanced"
capabilities = ["vision", "tool_use", "audio", "video", "pdf", "reasoning"]

[[models]]
id = "gemini-2.0-flash"
aliases = ["gemini-2.0-flash-exp"]
name = "Gemini 2.0 Flash"
provider = "google"
context_window = 1_048_576
max_output_tokens = 8_192
tier = "fast"
capabilities = ["vision", "tool_use", "audio", "video", "pdf"]

[[models]]
id = "gemini-1.5-pro"
aliases = ["gemini-1.5-pro-latest", "gemini-1.5-pro-002"]
name = "Gemini 1.5 Pro"
provider = "google"
context_window = 2_097_152
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["vision", "tool_use", "audio", "video", "pdf"]

[[models]]
id = "gemini-1.5-flash"
aliases = ["gemini-1.5-flash-latest", "gemini-1.5-flash-002"]
name = "Gemini 1.5 Flash"
provider = "google"
context_window = 1_048_576
max_output_tokens = 8_192
tier = "fast"
capabilities = ["vision", "tool_use", "audio", "video", "pdf"]

[[models]]
id = "gemini-1.0-pro"
aliases = ["gemini-pro"]
name = "Gemini 1.0 Pro"
provider = "google"
context_window = 32_768
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

# =============================================================================
# DeepSeek Models
# =============================================================================

[[models]]
id = "deepseek-v3"
aliases = ["deepseek-chat", "deepseek-v3.1"]
name = "DeepSeek V3"
provider = "deepseek"
context_window = 64_000
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "deepseek-coder"
aliases = ["deepseek-coder-v2"]
name = "DeepSeek Coder"
provider = "deepseek"
context_window = 64_000
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "deepseek-r1"
aliases = ["deepseek-reasoner"]
name = "DeepSeek R1"
provider = "deepseek"
context_window = 64_000
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["tool_use", "reasoning"]

# =============================================================================
# Mistral Models
# =============================================================================

[[models]]
id = "mistral-large"
aliases = ["mistral-large-latest", "mistral-large-2411"]
name = "Mistral Large"
provider = "mistral"
context_window = 128_000
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["vision", "tool_use"]

[[models]]
id = "mistral-medium"
aliases = ["mistral-medium-latest"]
name = "Mistral Medium"
provider = "mistral"
context_window = 32_768
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "mistral-small"
aliases = ["mistral-small-latest", "mistral-small-2409"]
name = "Mistral Small"
provider = "mistral"
context_window = 32_768
max_output_tokens = 8_192
tier = "fast"
capabilities = ["tool_use"]

[[models]]
id = "codestral"
aliases = ["codestral-latest", "codestral-2405"]
name = "Codestral"
provider = "mistral"
context_window = 32_768
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "pixtral-large"
aliases = ["pixtral-large-latest", "pixtral-large-2411"]
name = "Pixtral Large"
provider = "mistral"
context_window = 128_000
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["vision", "tool_use"]

# =============================================================================
# Meta Llama Models
# =============================================================================

[[models]]
id = "llama-3.3-70b"
aliases = ["llama-3.3-70b-instruct"]
name = "Llama 3.3 70B"
provider = "meta"
context_window = 128_000
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["tool_use"]

[[models]]
id = "llama-3.2-90b-vision"
aliases = ["llama-3.2-90b-vision-instruct"]
name = "Llama 3.2 90B Vision"
provider = "meta"
context_window = 128_000
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["vision", "tool_use"]

[[models]]
id = "llama-3.1-405b"
aliases = ["llama-3.1-405b-instruct"]
name = "Llama 3.1 405B"
provider = "meta"
context_window = 128_000
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["tool_use"]

[[models]]
id = "llama-3.1-70b"
aliases = ["llama-3.1-70b-instruct"]
name = "Llama 3.1 70B"
provider = "meta"
context_window = 128_000
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "llama-3.1-8b"
aliases = ["llama-3.1-8b-instruct"]
name = "Llama 3.1 8B"
provider = "meta"
context_window = 128_000
max_output_tokens = 8_192
tier = "fast"
capabilities = ["tool_use"]

# =============================================================================
# xAI Grok Models
# =============================================================================

[[models]]
id = "grok-2"
aliases = ["grok-2-1212"]
name = "Grok 2"
provider = "xai"
context_window = 131_072
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["vision", "tool_use"]

[[models]]
id = "grok-2-mini"
aliases = []
name = "Grok 2 Mini"
provider = "xai"
context_window = 131_072
max_output_tokens = 8_192
tier = "fast"
capabilities = ["tool_use"]

[[models]]
id = "grok-3"
aliases = []
name = "Grok 3"
provider = "xai"
context_window = 131_072
max_output_tokens = 16_384
tier = "flagship"
capabilities = ["vision", "tool_use", "reasoning"]

# =============================================================================
# Qwen Models
# =============================================================================

[[models]]
id = "qwen-2.5-72b"
aliases = ["qwen-2.5-72b-instruct", "qwen2.5-72b"]
name = "Qwen 2.5 72B"
provider = "alibaba"
context_window = 131_072
max_output_tokens = 8_192
tier = "flagship"
capabilities = ["tool_use"]

[[models]]
id = "qwen-2.5-32b"
aliases = ["qwen-2.5-32b-instruct", "qwen2.5-32b"]
name = "Qwen 2.5 32B"
provider = "alibaba"
context_window = 131_072
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "qwen-2.5-coder-32b"
aliases = ["qwen2.5-coder-32b-instruct"]
name = "Qwen 2.5 Coder 32B"
provider = "alibaba"
context_window = 131_072
max_output_tokens = 8_192
tier = "balanced"
capabilities = ["tool_use"]

[[models]]
id = "qwq-32b"
aliases = ["qwq-32b-preview"]
name = "QwQ 32B"
provider = "alibaba"
context_window = 131_072
max_output_tokens = 16_384
tier = "flagship"
capabilities = ["tool_use", "reasoning"]
